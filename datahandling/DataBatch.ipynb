{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5db8f3c4d2dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import threading\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import DataReader\n",
    "import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class example(object):\n",
    "    def __init__(self, article, abstract_sentences, vocab, hps):\n",
    "        self.hps = hps\n",
    "        start = vocab.word2id(data.START_DECODING)\n",
    "        stop = vocab.word2id(data.STOP_DECODING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-32cc7aefbcc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marticle_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_words\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mhps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_enc_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0marticle_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_enc_steps\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'article' is not defined"
     ]
    }
   ],
   "source": [
    "        article_words = article.split()\n",
    "        if len(article_words) > hps.max_enc_steps:\n",
    "            article_words = article_words[:hps.max_enc_steps]\n",
    "        self.enc_len = len(article_words) \n",
    "        self.enc_input = [vocab.word2id(w) for w in article_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abstract_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b53b08e1583f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mabstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabstract_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mabstract_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mabs_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mabstract_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'abstract_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "        abstract = ' '.join(abstract_sentences)\n",
    "        abstract_words = abstract.split()\n",
    "        abs_ids = [vocab.word2id(w) for w in abstract_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a8e48bc09a16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dec_inp_targ_seqs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_dec_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_decoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_decoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "        self.dec_input, self.target = self.get_dec_inp_targ_seqs(abs_ids, hps.max_dec_steps, start_decoding, stop_decoding)\n",
    "        self.dec_len = len(self.dec_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-afc89ecdb939>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mhps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpointer_gen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_input_extend_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marticle_oovs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marticle2ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mabs_ids_extend_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstract2ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabstract_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marticle_oovs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dec_inp_targ_seqs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs_ids_extend_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_dec_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_decoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_decoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginal_article\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hps' is not defined"
     ]
    }
   ],
   "source": [
    "        if hps.pointer_gen:\n",
    "            self.enc_input_extend_vocab, self.article_oovs = data.article2ids(article_words, vocab)\n",
    "            abs_ids_extend_vocab = data.abstract2ids(abstract_words, vocab, self.article_oovs)\n",
    "            _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, hps.max_dec_steps, start_decoding, stop_decoding)\n",
    "        self.original_article = article\n",
    "        self.original_abstract = abstract\n",
    "        self.original_abstract_sents = abstract_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_dec_input_target(self, sequence, max_len, start_id, stop_id):\n",
    "        inp = [start_id] + sequence[:]\n",
    "        target = sequence[:]\n",
    "        if len(inp) > max_len:\n",
    "            inp = inp[:max_len]\n",
    "            target = target[:max_len]\n",
    "        else:\n",
    "            target.append(stop_id)\n",
    "        assert len(inp) == len(target)\n",
    "        return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def pad_encoder_input(self, max_len, pad_id):\n",
    "        while len(self.enc_input) < max_len:\n",
    "            self.enc_input.append(pad_id)\n",
    "        if self.hps.pointer_gen:\n",
    "            while len(self.enc_input_extend_vocab) < max_len:\n",
    "                self.enc_input_extend_vocab.append(pad_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def pad_decoder_input_target(self, max_len, pad_id):\n",
    "        while len(self.dec_input) < max_len:\n",
    "            self.dec_input.append(pad_id)\n",
    "        while len(self.target) < max_len:\n",
    "            self.target.append(pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class batcher(object):\n",
    "    BATCH_QUEUE_MAX = 50\n",
    "    def __init__(self, data_path, vocab, hps, single_pass):\n",
    "        self._data_path = data_path\n",
    "        self._vocab = vocab\n",
    "        self._hps = hps\n",
    "        self._single_pass = single_pass\n",
    "        self._batch_queue = Queue.Queue(self.BATCH_QUEUE_MAX)\n",
    "        self._example_queue = Queue.Queue(self.BATCH_QUEUE_MAX * self._hps.batch_size)\n",
    "        self._num_example_q_threads = 1\n",
    "        self._num_batch_q_threads = 1\n",
    "        self._bucketing_cache_size = 1\n",
    "        self._finished_reading = False\n",
    "        self._example_q_threads = []\n",
    "        for _ in xrange(self._num_example_q_threads):\n",
    "            self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
    "            self._example_q_threads[-1].daemon = True\n",
    "            self._example_q_threads[-1].start()\n",
    "        self._batch_q_threads = []\n",
    "        for _ in xrange(self._num_batch_q_threads):\n",
    "            self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
    "            self._batch_q_threads[-1].daemon = True\n",
    "            self._batch_q_threads[-1].start()\n",
    "        if not single_pass:\n",
    "            self._watch_thread = Thread(target=self.watch_threads)\n",
    "            self._watch_thread.daemon = True\n",
    "            self._watch_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def next_batch(self):\n",
    "        if self._batch_queue.qsize() == 0:\n",
    "            tf.logging.warning('Bucket input queue empty calling next_batch.', self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "            if self._single_pass and self._finished_reading:\n",
    "                tf.logging.info(\"Finished reading dataset\")\n",
    "                return None\n",
    "        batch = self._batch_queue.get() # get the next Batch\n",
    "        return batch\n",
    "    def fill_example_queue(self):\n",
    "        input_gen = self.text_generator(data.example_generator(self._data_path, self._single_pass))\n",
    "        while True:\n",
    "            try:\n",
    "                (article, abstract) = input_gen.next() \n",
    "            except StopIteration:\n",
    "                tf.logging.info(\"exhausted data.\")\n",
    "                if self._single_pass:\n",
    "                    tf.logging.info(\"This thread is stopping.\")\n",
    "                    self._finished_reading = True\n",
    "                    break\n",
    "                else:\n",
    "                    raise Exception(\"single_pass mode is off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            abstract_sentences = [sent.strip() for sent in data.abstract2sents(abstract)]\n",
    "            example = Example(article, abstract_sentences, self._vocab, self._hps)\n",
    "            self._example_queue.put(example)\n",
    "            \n",
    "    def fill_batch_queue(self):\n",
    "        while True:\n",
    "            if self._hps.mode != 'decode':\n",
    "                inputs = []\n",
    "                for _ in xrange(self._hps.batch_size * self._bucketing_cache_size):\n",
    "                    inputs.append(self._example_queue.get())\n",
    "                inputs = sorted(inputs, key=lambda inp: inp.enc_len) # sort by length of encoder sequence\n",
    "                batches = []\n",
    "                for i in xrange(0, len(inputs), self._hps.batch_size):\n",
    "                    batches.append(inputs[i:i + self._hps.batch_size])\n",
    "                if not self._single_pass:\n",
    "                    shuffle(batches)\n",
    "                for b in batches:\n",
    "                    self._batch_queue.put(Batch(b, self._hps, self._vocab))\n",
    "            else:\n",
    "                ex = self._example_queue.get()\n",
    "                b = [ex for _ in xrange(self._hps.batch_size)]\n",
    "                self._batch_queue.put(Batch(b, self._hps, self._vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def watch_threads(self):\n",
    "        while True:\n",
    "            time.sleep(60)\n",
    "            for idx,t in enumerate(self._example_q_threads):\n",
    "                if not t.is_alive(): \n",
    "                    tf.logging.error(' Restarting.')\n",
    "                    new_t = Thread(target=self.fill_example_queue)\n",
    "                    self._example_q_threads[idx] = new_t\n",
    "                    new_t.daemon = True\n",
    "                    new_t.start()\n",
    "                    for idx,t in enumerate(self._batch_q_threads):\n",
    "                        if not t.is_alive():\n",
    "                            tf.logging.error('Restarting.')\n",
    "                            new_t = Thread(target=self.fill_batch_queue)\n",
    "                            self._batch_q_threads[idx] = new_t\n",
    "                            new_t.daemon = True\n",
    "                            new_t.start()\n",
    "    def text_generator(self, example_generator):\n",
    "        while True:\n",
    "            e = example_generator.next()\n",
    "            try:\n",
    "                article_text = e.features.feature['article'].bytes_list.value[0]\n",
    "                abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
    "            except ValueError:\n",
    "                tf.logging.error('Failed to get article or abstract')\n",
    "                continue\n",
    "            if len(article_text)==0:\n",
    "                tf.logging.warning('Found an empty article text')\n",
    "            else:\n",
    "                yield (article_text, abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class batch(object):\n",
    "    def __init__(self, example_list, hps, vocab):\n",
    "        self.pad_id = vocab.word2id(data.PAD_TOKEN)\n",
    "        self.init_encoder_seq(example_list, hps)\n",
    "        self.init_decoder_seq(example_list, hps)\n",
    "        self.store_orig_strings(example_list)\n",
    "    \n",
    "    def init_encoder_seq(self, example_list, hps):\n",
    "        max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
    "        for ex in example_list:\n",
    "            ex.pad_encoder_input(max_enc_seq_len, self.pad_id)\n",
    "        self.enc_batch = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "        self.enc_lens = np.zeros((hps.batch_size), dtype=np.int32)\n",
    "        self.enc_padding_mask = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.float32)\n",
    "        \n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.enc_batch[i, :] = ex.enc_input[:]\n",
    "            self.enc_lens[i] = ex.enc_len\n",
    "            for j in xrange(ex.enc_len):\n",
    "                self.enc_padding_mask[i][j] = 1\n",
    "                \n",
    "        if hps.pointer_gen:\n",
    "            self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
    "            self.art_oovs = [ex.article_oovs for ex in example_list]\n",
    "            self.enc_batch_extend_vocab = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "            for i, ex in enumerate(example_list):\n",
    "                self.enc_batch_extend_vocab[i, :] = ex.enc_input_extend_vocab[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def init_decoder_seq(self, example_list, hps):\n",
    "        for ex in example_list:\n",
    "            ex.pad_decoder_inp_targ(hps.max_dec_steps, self.pad_id)\n",
    "        self.dec_batch = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.int32)\n",
    "        self.target_batch = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.int32)\n",
    "        self.dec_padding_mask = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.float32)\n",
    "        \n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.dec_batch[i, :] = ex.dec_input[:]\n",
    "            self.target_batch[i, :] = ex.target[:]\n",
    "            for j in xrange(ex.dec_len):\n",
    "                self.dec_padding_mask[i][j] = 1\n",
    "            \n",
    "    def store_orig_strings(self, example_list):\n",
    "        self.original_articles = [ex.original_article for ex in example_list]\n",
    "        self.original_abstracts = [ex.original_abstract for ex in example_list]\n",
    "        self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
